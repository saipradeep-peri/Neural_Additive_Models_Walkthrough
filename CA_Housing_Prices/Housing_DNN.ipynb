{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Housing_DNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "91wRkjWjX8Ap"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "from time import time\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsgTlJRfYCI4",
        "outputId": "3b04bfe6-ddaa-46a8-d117-77001b8808da"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()\n",
        "housing_df = pd.DataFrame(data=housing.data, columns=housing.feature_names)\n",
        "housing_df['target'] = housing.target"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to /root/scikit_learn_data\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyNc566PYFzI"
      },
      "source": [
        "x = housing_df.drop('target', axis=1)\n",
        "y = housing_df[['target']]\n",
        "\n",
        "X, X_test, Y, y_test = train_test_split(x, y, test_size=0.1, random_state=2137)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.1, random_state=2137)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.to_numpy().astype('float32'))\n",
        "X_val = torch.from_numpy(X_val.to_numpy().astype('float32'))\n",
        "X_test = torch.from_numpy(X_test.to_numpy().astype('float32'))\n",
        "y_train = torch.from_numpy(y_train.to_numpy().reshape(-1, 1).astype('float32'))\n",
        "y_val = torch.from_numpy(y_val.to_numpy().reshape(-1, 1).astype('float32'))\n",
        "y_test = torch.from_numpy(y_test.to_numpy().reshape(-1, 1).astype('float32'))\n",
        "\n",
        "batch_size=1024\n",
        "dataset_train = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "dataset_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "dataset_val = torch.utils.data.TensorDataset(X_val, y_val)\n",
        "dataset_val = torch.utils.data.DataLoader(dataset_val, batch_size=32, shuffle=False)\n",
        "\n",
        "dataset_test = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "dataset_test = torch.utils.data.DataLoader(dataset_test, batch_size=32, shuffle=False)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv_feIgnYRAs"
      },
      "source": [
        "class DNN_model(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(DNN_model, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 128)\n",
        "        self.layer2 = nn.Linear(128, 64)\n",
        "        #self.layer3 = nn.Linear(32, 16)\n",
        "        self.layer4 = nn.Linear(64, 1)\n",
        "        #self.dropout1 = nn.Dropout(0.1)\n",
        "        #self.dropout2 = nn.Dropout(0.1)\n",
        "        #self.dropout3 = nn.Dropout(0.1)\n",
        "        self.act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.act(self.layer1(x))\n",
        "        #x = self.dropout1(x)\n",
        "        x = self.act(self.layer2(x))\n",
        "        #x = self.dropout2(x)\n",
        "        #x = F.relu(self.layer3(x))\n",
        "        #x = self.dropout3(x)\n",
        "        x = self.layer4(x)\n",
        "        return x\n"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_vyY2K2YaQ2"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR36eWtRYapp"
      },
      "source": [
        "model = DNN_model(len(dataset_train.dataset[0][0]))"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMEO6cB0YcfV"
      },
      "source": [
        "learning_rate = 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
        "scheduler = StepLR(optimizer, step_size=40, gamma=0.2)\n",
        "criterion=nn.MSELoss()"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zc0gyX7zYi4i"
      },
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    start_time = time()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
        "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
        "        data = data.to(device)\n",
        "        target = target.to(device) # all data & model on same device\n",
        "\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs.squeeze(), target.squeeze())\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    end_time = time()\n",
        "    \n",
        "    running_loss /= len(train_loader)\n",
        "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
        "    return running_loss\n",
        "\n",
        "def test_model(model, test_loader, criterion):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "        Y_pred, Y = [], []\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):   \n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs.squeeze(), target.squeeze()).detach()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            pred = outputs.cpu().detach()\n",
        "\n",
        "            Y_pred.append(pred.squeeze())\n",
        "            Y.append(target.squeeze().cpu())\n",
        "\n",
        "        running_loss /= len(test_loader)\n",
        "        Y_pred = torch.cat(Y_pred)\n",
        "        Y = torch.cat(Y)\n",
        "        r2 = r2_score(Y, Y_pred)\n",
        "        print(\"Testing r2_score : \"+str(i)+\" :\",r2)\n",
        "        print('Testing Loss: ', running_loss)\n",
        "        return running_loss, r2\n"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAOT5TosYueA",
        "outputId": "2bab8430-af5a-4452-906f-eadaf5155547"
      },
      "source": [
        "n_epochs = 100\n",
        "for i in range(n_epochs):\n",
        "    print(\"epoch no:\", i)\n",
        "    train_loss = train_epoch(model, dataset_train, criterion, optimizer)\n",
        "    test_loss, test_acc = test_model(model, dataset_val, criterion)\n",
        "    scheduler.step()\n"
      ],
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch no: 0\n",
            "Training Loss:  2.3642242936527027 Time:  0.242279052734375 s\n",
            "Testing r2_score : 0 : -0.018066223630313738\n",
            "Testing Loss:  1.3011857483346583\n",
            "epoch no: 1\n",
            "Training Loss:  1.3736993565278894 Time:  0.21291017532348633 s\n",
            "Testing r2_score : 1 : -0.03809797038193219\n",
            "Testing Loss:  1.3373406862808486\n",
            "epoch no: 2\n",
            "Training Loss:  1.3585059011683744 Time:  0.2906637191772461 s\n",
            "Testing r2_score : 2 : 0.0015621487941418488\n",
            "Testing Loss:  1.2791948065919392\n",
            "epoch no: 3\n",
            "Training Loss:  1.34118749814875 Time:  0.2106339931488037 s\n",
            "Testing r2_score : 3 : 0.002846576306152815\n",
            "Testing Loss:  1.2779142684855704\n",
            "epoch no: 4\n",
            "Training Loss:  1.340509379611296 Time:  0.21464276313781738 s\n",
            "Testing r2_score : 4 : 0.004256990280688933\n",
            "Testing Loss:  1.276786600129079\n",
            "epoch no: 5\n",
            "Training Loss:  1.3404129042344934 Time:  0.22380876541137695 s\n",
            "Testing r2_score : 5 : 0.007405852736923402\n",
            "Testing Loss:  1.2724721613576857\n",
            "epoch no: 6\n",
            "Training Loss:  1.3240591638228472 Time:  0.30002617835998535 s\n",
            "Testing r2_score : 6 : 0.009539395423569408\n",
            "Testing Loss:  1.2694611711017156\n",
            "epoch no: 7\n",
            "Training Loss:  1.3252122402191162 Time:  0.21982908248901367 s\n",
            "Testing r2_score : 7 : 0.012897231683100308\n",
            "Testing Loss:  1.2651815818527998\n",
            "epoch no: 8\n",
            "Training Loss:  1.3241146943148445 Time:  0.2085256576538086 s\n",
            "Testing r2_score : 8 : 0.018231576402385707\n",
            "Testing Loss:  1.258018722978689\n",
            "epoch no: 9\n",
            "Training Loss:  1.3103215343811934 Time:  0.28122401237487793 s\n",
            "Testing r2_score : 9 : 0.027290072862400816\n",
            "Testing Loss:  1.2465063828532978\n",
            "epoch no: 10\n",
            "Training Loss:  1.3054764901890474 Time:  0.2131960391998291 s\n",
            "Testing r2_score : 10 : 0.04239701751363256\n",
            "Testing Loss:  1.2278121214801982\n",
            "epoch no: 11\n",
            "Training Loss:  1.285438993397881 Time:  0.2827882766723633 s\n",
            "Testing r2_score : 11 : 0.06293054929637243\n",
            "Testing Loss:  1.200653644941621\n",
            "epoch no: 12\n",
            "Training Loss:  1.2609542608261108 Time:  0.20811748504638672 s\n",
            "Testing r2_score : 12 : 0.0831343554922066\n",
            "Testing Loss:  1.1746827636734913\n",
            "epoch no: 13\n",
            "Training Loss:  1.2294026402866138 Time:  0.21218228340148926 s\n",
            "Testing r2_score : 13 : 0.11088140435564386\n",
            "Testing Loss:  1.1390141673007255\n",
            "epoch no: 14\n",
            "Training Loss:  1.1967596096151016 Time:  0.2791881561279297 s\n",
            "Testing r2_score : 14 : 0.13587582634776552\n",
            "Testing Loss:  1.1083658771999811\n",
            "epoch no: 15\n",
            "Training Loss:  1.140628744574154 Time:  0.21177434921264648 s\n",
            "Testing r2_score : 15 : 0.1873385373980574\n",
            "Testing Loss:  1.0425015905145871\n",
            "epoch no: 16\n",
            "Training Loss:  1.091015735093285 Time:  0.21098542213439941 s\n",
            "Testing r2_score : 16 : 0.23622511736570395\n",
            "Testing Loss:  0.9791434796179755\n",
            "epoch no: 17\n",
            "Training Loss:  1.0090800769188826 Time:  0.22253179550170898 s\n",
            "Testing r2_score : 17 : 0.31451665838316833\n",
            "Testing Loss:  0.8812740450188264\n",
            "epoch no: 18\n",
            "Training Loss:  0.9141118105720071 Time:  0.28525424003601074 s\n",
            "Testing r2_score : 18 : 0.3580414487131359\n",
            "Testing Loss:  0.8319875789900958\n",
            "epoch no: 19\n",
            "Training Loss:  0.823381690418019 Time:  0.21127986907958984 s\n",
            "Testing r2_score : 19 : 0.4225666782641838\n",
            "Testing Loss:  0.7524199243319236\n",
            "epoch no: 20\n",
            "Training Loss:  0.7387069042991189 Time:  0.2158055305480957 s\n",
            "Testing r2_score : 20 : 0.5220321574052675\n",
            "Testing Loss:  0.6171858992617009\n",
            "epoch no: 21\n",
            "Training Loss:  0.6783829892382902 Time:  0.29164838790893555 s\n",
            "Testing r2_score : 21 : 0.558094558139341\n",
            "Testing Loss:  0.5729257108296378\n",
            "epoch no: 22\n",
            "Training Loss:  0.6293721023727866 Time:  0.20548009872436523 s\n",
            "Testing r2_score : 22 : 0.588216968501152\n",
            "Testing Loss:  0.5352319660833327\n",
            "epoch no: 23\n",
            "Training Loss:  0.6068094092256883 Time:  0.21403241157531738 s\n",
            "Testing r2_score : 23 : 0.5964796100299461\n",
            "Testing Loss:  0.5276262484869715\n",
            "epoch no: 24\n",
            "Training Loss:  0.5947813076131484 Time:  0.2889242172241211 s\n",
            "Testing r2_score : 24 : 0.609115372706899\n",
            "Testing Loss:  0.5051604330034579\n",
            "epoch no: 25\n",
            "Training Loss:  0.5744125282063204 Time:  0.22707748413085938 s\n",
            "Testing r2_score : 25 : 0.6274303668235809\n",
            "Testing Loss:  0.484299566786168\n",
            "epoch no: 26\n",
            "Training Loss:  0.5583605538396275 Time:  0.2178804874420166 s\n",
            "Testing r2_score : 26 : 0.6355008220823524\n",
            "Testing Loss:  0.4737856355258974\n",
            "epoch no: 27\n",
            "Training Loss:  0.5441074634299559 Time:  0.27835655212402344 s\n",
            "Testing r2_score : 27 : 0.6407914752865533\n",
            "Testing Loss:  0.4675840200003931\n",
            "epoch no: 28\n",
            "Training Loss:  0.5402484395924736 Time:  0.20313286781311035 s\n",
            "Testing r2_score : 28 : 0.6428647028888452\n",
            "Testing Loss:  0.4644535181380935\n",
            "epoch no: 29\n",
            "Training Loss:  0.5344808522392722 Time:  0.20819830894470215 s\n",
            "Testing r2_score : 29 : 0.6511112685624876\n",
            "Testing Loss:  0.4540223544937069\n",
            "epoch no: 30\n",
            "Training Loss:  0.5357846354737001 Time:  0.28597426414489746 s\n",
            "Testing r2_score : 30 : 0.5963790185618967\n",
            "Testing Loss:  0.5169922315468223\n",
            "epoch no: 31\n",
            "Training Loss:  0.5361029453137341 Time:  0.21120858192443848 s\n",
            "Testing r2_score : 31 : 0.6554373466494607\n",
            "Testing Loss:  0.446945439960997\n",
            "epoch no: 32\n",
            "Training Loss:  0.5308918479610892 Time:  0.21672654151916504 s\n",
            "Testing r2_score : 32 : 0.652693157988328\n",
            "Testing Loss:  0.4489097395690821\n",
            "epoch no: 33\n",
            "Training Loss:  0.5203817504293778 Time:  0.2903876304626465 s\n",
            "Testing r2_score : 33 : 0.6622482169932183\n",
            "Testing Loss:  0.4385790731442177\n",
            "epoch no: 34\n",
            "Training Loss:  0.5183699341381297 Time:  0.21012306213378906 s\n",
            "Testing r2_score : 34 : 0.6567684704016155\n",
            "Testing Loss:  0.4475104038493108\n",
            "epoch no: 35\n",
            "Training Loss:  0.5195950760560877 Time:  0.2073531150817871 s\n",
            "Testing r2_score : 35 : 0.6554821694216656\n",
            "Testing Loss:  0.4493053838358087\n",
            "epoch no: 36\n",
            "Training Loss:  0.5299392658121446 Time:  0.2868466377258301 s\n",
            "Testing r2_score : 36 : 0.659932158385225\n",
            "Testing Loss:  0.4431212428262678\n",
            "epoch no: 37\n",
            "Training Loss:  0.5070271158919615 Time:  0.2021923065185547 s\n",
            "Testing r2_score : 37 : 0.6666625611927628\n",
            "Testing Loss:  0.4310932560997494\n",
            "epoch no: 38\n",
            "Training Loss:  0.5039603026474223 Time:  0.26991724967956543 s\n",
            "Testing r2_score : 38 : 0.6588322799433327\n",
            "Testing Loss:  0.44498526191307325\n",
            "epoch no: 39\n",
            "Training Loss:  0.5107990573434269 Time:  0.19952821731567383 s\n",
            "Testing r2_score : 39 : 0.6685961277177719\n",
            "Testing Loss:  0.43099461218058055\n",
            "epoch no: 40\n",
            "Training Loss:  0.5017227916156545 Time:  0.22763347625732422 s\n",
            "Testing r2_score : 40 : 0.6708401190931417\n",
            "Testing Loss:  0.42768039668010455\n",
            "epoch no: 41\n",
            "Training Loss:  0.49581296303692984 Time:  0.27901315689086914 s\n",
            "Testing r2_score : 41 : 0.6740635124926881\n",
            "Testing Loss:  0.42294608511156956\n",
            "epoch no: 42\n",
            "Training Loss:  0.4942965893184437 Time:  0.2025918960571289 s\n",
            "Testing r2_score : 42 : 0.6714203182169416\n",
            "Testing Loss:  0.4269520029173059\n",
            "epoch no: 43\n",
            "Training Loss:  0.49832886632751017 Time:  0.2001638412475586 s\n",
            "Testing r2_score : 43 : 0.6746883739475109\n",
            "Testing Loss:  0.4221035464335296\n",
            "epoch no: 44\n",
            "Training Loss:  0.49514157456510205 Time:  0.30396127700805664 s\n",
            "Testing r2_score : 44 : 0.6747102541685355\n",
            "Testing Loss:  0.42125105933617735\n",
            "epoch no: 45\n",
            "Training Loss:  0.5005642947028665 Time:  0.21344947814941406 s\n",
            "Testing r2_score : 45 : 0.6729615220892052\n",
            "Testing Loss:  0.42281451745558596\n",
            "epoch no: 46\n",
            "Training Loss:  0.4918385270763846 Time:  0.20221185684204102 s\n",
            "Testing r2_score : 46 : 0.6754030119473527\n",
            "Testing Loss:  0.42033440177723513\n",
            "epoch no: 47\n",
            "Training Loss:  0.4931335642057307 Time:  0.2802271842956543 s\n",
            "Testing r2_score : 47 : 0.6755394262646577\n",
            "Testing Loss:  0.42100826739254643\n",
            "epoch no: 48\n",
            "Training Loss:  0.4955001736388487 Time:  0.21283721923828125 s\n",
            "Testing r2_score : 48 : 0.6762672929383774\n",
            "Testing Loss:  0.4193377775155892\n",
            "epoch no: 49\n",
            "Training Loss:  0.4940907797392677 Time:  0.20969843864440918 s\n",
            "Testing r2_score : 49 : 0.6741257790961118\n",
            "Testing Loss:  0.42321771004442443\n",
            "epoch no: 50\n",
            "Training Loss:  0.49348785070812 Time:  0.2733800411224365 s\n",
            "Testing r2_score : 50 : 0.676858138535301\n",
            "Testing Loss:  0.4189597635956134\n",
            "epoch no: 51\n",
            "Training Loss:  0.48988190118004293 Time:  0.21106624603271484 s\n",
            "Testing r2_score : 51 : 0.675865290754264\n",
            "Testing Loss:  0.41926658759682867\n",
            "epoch no: 52\n",
            "Training Loss:  0.4931586840573479 Time:  0.2203986644744873 s\n",
            "Testing r2_score : 52 : 0.676933884898535\n",
            "Testing Loss:  0.4190742052712683\n",
            "epoch no: 53\n",
            "Training Loss:  0.4929800875046674 Time:  0.21066856384277344 s\n",
            "Testing r2_score : 53 : 0.6772617326921402\n",
            "Testing Loss:  0.4184861281665705\n",
            "epoch no: 54\n",
            "Training Loss:  0.489169857081245 Time:  0.27971911430358887 s\n",
            "Testing r2_score : 54 : 0.6719732669800187\n",
            "Testing Loss:  0.42655727469314964\n",
            "epoch no: 55\n",
            "Training Loss:  0.4901130742886487 Time:  0.2079169750213623 s\n",
            "Testing r2_score : 55 : 0.6771078000096986\n",
            "Testing Loss:  0.4189529108293986\n",
            "epoch no: 56\n",
            "Training Loss:  0.49150079138138714 Time:  0.21231603622436523 s\n",
            "Testing r2_score : 56 : 0.67135074520772\n",
            "Testing Loss:  0.42741065838579406\n",
            "epoch no: 57\n",
            "Training Loss:  0.490511871436063 Time:  0.3039250373840332 s\n",
            "Testing r2_score : 57 : 0.6773597177594559\n",
            "Testing Loss:  0.4186506092043246\n",
            "epoch no: 58\n",
            "Training Loss:  0.4899553323493284 Time:  0.20252013206481934 s\n",
            "Testing r2_score : 58 : 0.6758897429627988\n",
            "Testing Loss:  0.4186650608050621\n",
            "epoch no: 59\n",
            "Training Loss:  0.4916298722519594 Time:  0.21279525756835938 s\n",
            "Testing r2_score : 59 : 0.6779230746185605\n",
            "Testing Loss:  0.41631748792478596\n",
            "epoch no: 60\n",
            "Training Loss:  0.4896338897592881 Time:  0.2841958999633789 s\n",
            "Testing r2_score : 60 : 0.6791213848308015\n",
            "Testing Loss:  0.41568649118229495\n",
            "epoch no: 61\n",
            "Training Loss:  0.4915638548486373 Time:  0.22206354141235352 s\n",
            "Testing r2_score : 61 : 0.6796314507106009\n",
            "Testing Loss:  0.4151619048946995\n",
            "epoch no: 62\n",
            "Training Loss:  0.48547909540288586 Time:  0.2725386619567871 s\n",
            "Testing r2_score : 62 : 0.6797231420198495\n",
            "Testing Loss:  0.414848366278713\n",
            "epoch no: 63\n",
            "Training Loss:  0.48549038872999306 Time:  0.21257758140563965 s\n",
            "Testing r2_score : 63 : 0.6801619038401429\n",
            "Testing Loss:  0.4144073462082168\n",
            "epoch no: 64\n",
            "Training Loss:  0.485402491162805 Time:  0.20482683181762695 s\n",
            "Testing r2_score : 64 : 0.6802883320089739\n",
            "Testing Loss:  0.41372513644776104\n",
            "epoch no: 65\n",
            "Training Loss:  0.48442819188622865 Time:  0.2723727226257324 s\n",
            "Testing r2_score : 65 : 0.6754666697773664\n",
            "Testing Loss:  0.42151450050079214\n",
            "epoch no: 66\n",
            "Training Loss:  0.4862627439639148 Time:  0.20445752143859863 s\n",
            "Testing r2_score : 66 : 0.677494959253417\n",
            "Testing Loss:  0.4190653448892852\n",
            "epoch no: 67\n",
            "Training Loss:  0.4868929421200472 Time:  0.22001361846923828 s\n",
            "Testing r2_score : 67 : 0.6798954756305295\n",
            "Testing Loss:  0.4147105901423147\n",
            "epoch no: 68\n",
            "Training Loss:  0.48610977565540986 Time:  0.2732508182525635 s\n",
            "Testing r2_score : 68 : 0.6796417796631865\n",
            "Testing Loss:  0.41590795426045435\n",
            "epoch no: 69\n",
            "Training Loss:  0.4857374359579647 Time:  0.21017932891845703 s\n",
            "Testing r2_score : 69 : 0.6812291677471638\n",
            "Testing Loss:  0.41249879361209224\n",
            "epoch no: 70\n",
            "Training Loss:  0.483727283337537 Time:  0.21211957931518555 s\n",
            "Testing r2_score : 70 : 0.6818287589950424\n",
            "Testing Loss:  0.41180481016635895\n",
            "epoch no: 71\n",
            "Training Loss:  0.4811617241186254 Time:  0.2924342155456543 s\n",
            "Testing r2_score : 71 : 0.6819477345730784\n",
            "Testing Loss:  0.41187409450442103\n",
            "epoch no: 72\n",
            "Training Loss:  0.4836886349846335 Time:  0.21279430389404297 s\n",
            "Testing r2_score : 72 : 0.6794796490230772\n",
            "Testing Loss:  0.41610304278842475\n",
            "epoch no: 73\n",
            "Training Loss:  0.48407614932340737 Time:  0.2219846248626709 s\n",
            "Testing r2_score : 73 : 0.6808217003397989\n",
            "Testing Loss:  0.41238536824614314\n",
            "epoch no: 74\n",
            "Training Loss:  0.4849710236577427 Time:  0.29125404357910156 s\n",
            "Testing r2_score : 74 : 0.6822969005636796\n",
            "Testing Loss:  0.4109720812510636\n",
            "epoch no: 75\n",
            "Training Loss:  0.48095258369165306 Time:  0.22532129287719727 s\n",
            "Testing r2_score : 75 : 0.682543189119956\n",
            "Testing Loss:  0.4105617621187436\n",
            "epoch no: 76\n",
            "Training Loss:  0.4801188076243681 Time:  0.20884108543395996 s\n",
            "Testing r2_score : 76 : 0.6829923528434757\n",
            "Testing Loss:  0.4104462781194913\n",
            "epoch no: 77\n",
            "Training Loss:  0.48511113664683175 Time:  0.27654409408569336 s\n",
            "Testing r2_score : 77 : 0.6832962748754101\n",
            "Testing Loss:  0.4098951167474359\n",
            "epoch no: 78\n",
            "Training Loss:  0.48094404970898347 Time:  0.20961713790893555 s\n",
            "Testing r2_score : 78 : 0.6824573572645198\n",
            "Testing Loss:  0.4114660577753843\n",
            "epoch no: 79\n",
            "Training Loss:  0.47817331026582155 Time:  0.21499228477478027 s\n",
            "Testing r2_score : 79 : 0.6818110762988938\n",
            "Testing Loss:  0.4109324509814634\n",
            "epoch no: 80\n",
            "Training Loss:  0.4788755914744209 Time:  0.2065422534942627 s\n",
            "Testing r2_score : 80 : 0.6833952770619831\n",
            "Testing Loss:  0.40983757679745303\n",
            "epoch no: 81\n",
            "Training Loss:  0.47958169789875255 Time:  0.27588772773742676 s\n",
            "Testing r2_score : 81 : 0.683774190160093\n",
            "Testing Loss:  0.40906309475333\n",
            "epoch no: 82\n",
            "Training Loss:  0.48120837877778444 Time:  0.21712851524353027 s\n",
            "Testing r2_score : 82 : 0.6837523113855397\n",
            "Testing Loss:  0.40921273297172484\n",
            "epoch no: 83\n",
            "Training Loss:  0.482257885091445 Time:  0.2150585651397705 s\n",
            "Testing r2_score : 83 : 0.6838612595830695\n",
            "Testing Loss:  0.4092618732129113\n",
            "epoch no: 84\n",
            "Training Loss:  0.4825359705616446 Time:  0.29033970832824707 s\n",
            "Testing r2_score : 84 : 0.6830451343384284\n",
            "Testing Loss:  0.4106152565297434\n",
            "epoch no: 85\n",
            "Training Loss:  0.48338933376704946 Time:  0.21344733238220215 s\n",
            "Testing r2_score : 85 : 0.683440221539351\n",
            "Testing Loss:  0.41008673911377536\n",
            "epoch no: 86\n",
            "Training Loss:  0.4791327960350934 Time:  0.2937593460083008 s\n",
            "Testing r2_score : 86 : 0.6839317120252397\n",
            "Testing Loss:  0.4091957024598526\n",
            "epoch no: 87\n",
            "Training Loss:  0.479637193329194 Time:  0.20215845108032227 s\n",
            "Testing r2_score : 87 : 0.6838209754862363\n",
            "Testing Loss:  0.4094113331224959\n",
            "epoch no: 88\n",
            "Training Loss:  0.47577685468337116 Time:  0.20708942413330078 s\n",
            "Testing r2_score : 88 : 0.6839549296278982\n",
            "Testing Loss:  0.4091783120470532\n",
            "epoch no: 89\n",
            "Training Loss:  0.47928086273810444 Time:  0.28778934478759766 s\n",
            "Testing r2_score : 89 : 0.6839450175467849\n",
            "Testing Loss:  0.4086674269983324\n",
            "epoch no: 90\n",
            "Training Loss:  0.4823329185738283 Time:  0.2089521884918213 s\n",
            "Testing r2_score : 90 : 0.6840158833463497\n",
            "Testing Loss:  0.40872223043845873\n",
            "epoch no: 91\n",
            "Training Loss:  0.4825381636619568 Time:  0.2147045135498047 s\n",
            "Testing r2_score : 91 : 0.6842104844631178\n",
            "Testing Loss:  0.4086922801147073\n",
            "epoch no: 92\n",
            "Training Loss:  0.48493145143284516 Time:  0.20911526679992676 s\n",
            "Testing r2_score : 92 : 0.6839891188439465\n",
            "Testing Loss:  0.40906601235017936\n",
            "epoch no: 93\n",
            "Training Loss:  0.4825332901057075 Time:  0.29627418518066406 s\n",
            "Testing r2_score : 93 : 0.684282422751872\n",
            "Testing Loss:  0.40842328763614266\n",
            "epoch no: 94\n",
            "Training Loss:  0.47828375942566814 Time:  0.21045470237731934 s\n",
            "Testing r2_score : 94 : 0.6838182920519116\n",
            "Testing Loss:  0.4095229170585083\n",
            "epoch no: 95\n",
            "Training Loss:  0.48011551183812756 Time:  0.20730137825012207 s\n",
            "Testing r2_score : 95 : 0.6841158858132828\n",
            "Testing Loss:  0.40892561007354217\n",
            "epoch no: 96\n",
            "Training Loss:  0.4808892362258014 Time:  0.2762455940246582 s\n",
            "Testing r2_score : 96 : 0.6842041383892636\n",
            "Testing Loss:  0.4086026193226798\n",
            "epoch no: 97\n",
            "Training Loss:  0.47766245638622956 Time:  0.21135807037353516 s\n",
            "Testing r2_score : 97 : 0.6844586416774987\n",
            "Testing Loss:  0.4083885040323613\n",
            "epoch no: 98\n",
            "Training Loss:  0.48028388093499574 Time:  0.2894625663757324 s\n",
            "Testing r2_score : 98 : 0.6843983159902242\n",
            "Testing Loss:  0.4084655569266465\n",
            "epoch no: 99\n",
            "Training Loss:  0.4759735009249519 Time:  0.20168423652648926 s\n",
            "Testing r2_score : 99 : 0.6842219948959374\n",
            "Testing Loss:  0.4087499213420739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-iGRcr1bBy4"
      },
      "source": [
        "def cal_accuracy(y_test, y_pred):\n",
        "    print(\"r2_score : \", r2_score(y_test, y_pred))\n",
        "    print(\"MSE :\", mean_squared_error(y_test, y_pred))"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rsuc64vltxt",
        "outputId": "1a654b70-a0e8-433d-bb5f-0a8daf42c88c"
      },
      "source": [
        "model.eval()\n",
        "Y_pred, Y = [], []\n",
        "for features, targets in dataset_test:\n",
        "  y_pred = model(features).cpu().detach()\n",
        "  Y_pred.append(y_pred.squeeze())\n",
        "  Y.append(targets.squeeze())\n",
        "Y_pred = torch.cat(Y_pred)\n",
        "Y = torch.cat(Y)\n",
        "print(\"r2 of model :\", r2_score(np.array(Y),np.array(Y_pred)))\n",
        "print(\"MSE of the model :\", criterion(Y, Y_pred))"
      ],
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r2 of model : 0.6437082590688332\n",
            "MSE of the model : tensor(0.4624)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUr0hUtIl2sf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}