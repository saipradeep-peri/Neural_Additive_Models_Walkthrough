{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "DSLuvpvKvkPl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJa-AtD0QHzp"
   },
   "outputs": [],
   "source": [
    "# filter dplyr warnings\n",
    "%load_ext rpy2.ipython\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJuVT-cNE0Fe",
    "outputId": "97e92203-94c9-426c-90bc-076cc4180827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  age      race_factor gender_factor score_factor priors_count length_of_stay\n",
      "1  69            Other          Male     LowScore            0              1\n",
      "2  34 African-American          Male     LowScore            0             10\n",
      "3  24 African-American          Male     LowScore            4              1\n",
      "4  44            Other          Male     LowScore            0              1\n",
      "5  41        Caucasian          Male    HighScore           14              6\n",
      "6  43            Other          Male     LowScore            3              1\n",
      "  crime_factor\n",
      "1            F\n",
      "2            F\n",
      "3            F\n",
      "4            M\n",
      "5            F\n",
      "6            F\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(dplyr)\n",
    "library(ggplot2)\n",
    "raw_data <- read.csv(\"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\")\n",
    "nrow(raw_data)\n",
    "\n",
    "df <- dplyr::select(raw_data, age, c_charge_degree, race, age_cat, score_text, sex, priors_count, \n",
    "                    days_b_screening_arrest, decile_score, is_recid, two_year_recid, c_jail_in, c_jail_out) %>% \n",
    "        filter(days_b_screening_arrest <= 30) %>%\n",
    "        filter(days_b_screening_arrest >= -30) %>%\n",
    "        filter(is_recid != -1) %>%\n",
    "        filter(c_charge_degree != \"O\") %>%\n",
    "        filter(score_text != 'N/A')\n",
    "nrow(df)\n",
    "\n",
    "df$length_of_stay <- as.numeric(as.Date(df$c_jail_out) - as.Date(df$c_jail_in))\n",
    "\n",
    "df <- mutate(df, crime_factor = factor(c_charge_degree)) %>%\n",
    "      mutate(age_factor = as.factor(age_cat)) %>%\n",
    "      within(age_factor <- relevel(age_factor, ref = 1)) %>%\n",
    "      mutate(race_factor = factor(race)) %>%\n",
    "      within(race_factor <- relevel(race_factor, ref = 3)) %>%\n",
    "      mutate(gender_factor = factor(sex, labels= c(\"Female\",\"Male\"))) %>%\n",
    "      within(gender_factor <- relevel(gender_factor, ref = 2)) %>%\n",
    "      mutate(score_factor = factor(score_text != \"Low\", labels = c(\"LowScore\",\"HighScore\")))\n",
    "\n",
    "df <- df %>% select('age', 'race_factor', 'gender_factor', 'score_factor', 'priors_count', 'length_of_stay', 'crime_factor')\n",
    "\n",
    "df %>% head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "8vtuQNKctVym",
    "outputId": "9ba47283-de5a-4519-e137-08b1b42a7929"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>race_factor</th>\n",
       "      <th>gender_factor</th>\n",
       "      <th>score_factor</th>\n",
       "      <th>priors_count</th>\n",
       "      <th>length_of_stay</th>\n",
       "      <th>crime_factor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>Other</td>\n",
       "      <td>Male</td>\n",
       "      <td>LowScore</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Male</td>\n",
       "      <td>LowScore</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>African-American</td>\n",
       "      <td>Male</td>\n",
       "      <td>LowScore</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>Other</td>\n",
       "      <td>Male</td>\n",
       "      <td>LowScore</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>41</td>\n",
       "      <td>Caucasian</td>\n",
       "      <td>Male</td>\n",
       "      <td>HighScore</td>\n",
       "      <td>14</td>\n",
       "      <td>6.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age       race_factor  ... length_of_stay crime_factor\n",
       "1   69             Other  ...            1.0            F\n",
       "2   34  African-American  ...           10.0            F\n",
       "3   24  African-American  ...            1.0            F\n",
       "4   44             Other  ...            1.0            M\n",
       "5   41         Caucasian  ...            6.0            F\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compas_df_categorical = %R df\n",
    "compas_df = %R df\n",
    "compas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5n2AH3OhtqF4"
   },
   "outputs": [],
   "source": [
    "cat_features = ['race_factor','gender_factor', 'crime_factor']\n",
    "numeric_features = ['age','priors_count', 'length_of_stay']\n",
    "target_feature = [\"score_factor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ds_-iD1fOtT8"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "ly = LabelEncoder()\n",
    "\n",
    "compas_df['race_factor'] = ly.fit_transform(compas_df['race_factor'])\n",
    "race_mapping = dict(zip(ly.classes_, range(0, len(ly.classes_)+1)))\n",
    "compas_df['gender_factor'] = ly.fit_transform(compas_df['gender_factor'])\n",
    "gender_mapping = dict(zip(ly.classes_, range(0, len(ly.classes_)+1)))\n",
    "compas_df['crime_factor'] = ly.fit_transform(compas_df['crime_factor'])\n",
    "crime_factor_mapping = dict(zip(ly.classes_, range(0, len(ly.classes_)+1)))\n",
    "\n",
    "# compas_df['score_factor'] = ly.fit_transform(compas_df['score_factor'])\n",
    "# score_factor_mapping = dict(zip(ly.classes_, range(0, len(ly.classes_)+1)))\n",
    "\n",
    "compas_df['score_factor'] = compas_df['score_factor'].map({'HighScore' : 1, 'LowScore': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "-iZAXNwuOYRz"
   },
   "outputs": [],
   "source": [
    "x = compas_df[cat_features+numeric_features]\n",
    "y = compas_df[target_feature]\n",
    "\n",
    "X, X_test, Y, y_test = train_test_split(x, y, test_size=0.1, random_state=2137)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.1, random_state=2137)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.to_numpy().astype('float32'))\n",
    "X_val = torch.from_numpy(X_val.to_numpy().astype('float32'))\n",
    "X_test = torch.from_numpy(X_test.to_numpy().astype('float32'))\n",
    "y_train = torch.from_numpy(y_train.to_numpy().reshape(-1, 1).astype('float32'))\n",
    "y_val = torch.from_numpy(y_val.to_numpy().reshape(-1, 1).astype('float32'))\n",
    "y_test = torch.from_numpy(y_test.to_numpy().reshape(-1, 1).astype('float32'))\n",
    "\n",
    "batch_size=1024\n",
    "dataset_train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "dataset_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataset_val = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "dataset_val = torch.utils.data.DataLoader(dataset_val, batch_size=32, shuffle=False)\n",
    "\n",
    "dataset_test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "dataset_test = torch.utils.data.DataLoader(dataset_test, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "RuG6ttoKer6C"
   },
   "outputs": [],
   "source": [
    "features, target = next(iter(dataset_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8wPVGvQVexr_",
    "outputId": "8250a5c0-064d-4757-f5c8-c4c741035e1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 160,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mttTfTsypwB6"
   },
   "source": [
    "# DNN Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Qly5Gnl6MbFt"
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "\n",
    "  '''\n",
    "    binary_class:\n",
    "      True - Output single probability of positive class for BCELoss\n",
    "      False - Output probabilities for each class (positive/negative)\n",
    "  '''\n",
    "  def __init__(self, in_features, hidden=[128,128], act=nn.ReLU, binary_class=True):\n",
    "    super(DNN, self).__init__()\n",
    "    sizes = [in_features] + hidden\n",
    "    if binary_class:\n",
    "      sizes += [1]\n",
    "    else:\n",
    "      sizes += [2]\n",
    "    \n",
    "    layers = []\n",
    "    for i in range(len(sizes)-1):\n",
    "      layers.append(nn.Linear(sizes[i], sizes[i+1]))\n",
    "      if i != len(sizes)-2:\n",
    "        layers.append(act())\n",
    "        # layers.append(nn.Dropout(0.3))\n",
    "\n",
    "    for layer in layers:\n",
    "      if isinstance(layer, nn.Linear):\n",
    "          if isinstance(act, nn.Sigmoid):\n",
    "              init.xavier_normal_(layer.weight)\n",
    "              init.zeros_(layer.bias)\n",
    "          elif isinstance(act, nn.ReLU) or isinstance(act, nn.LeakyReLU):\n",
    "              init.kaiming_normal_(layer.weight)\n",
    "              init.zeros_(layer.bias)\n",
    "\n",
    "    self.layers = nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, X):\n",
    "    out = self.layers(X)\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "oyfVvh2EY6Tr"
   },
   "outputs": [],
   "source": [
    "class DNN_model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(DNN_model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "                           \n",
    "    def get_weights(self):\n",
    "        return self.weight\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = F.sigmoid(self.fc2(out)) #sigmoid as we use BCELoss\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnYA3OwUA29o",
    "outputId": "76ec6bfe-74e5-4ca4-c505-c505f4fee2d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'race_factor', 'gender_factor', 'score_factor', 'priors_count',\n",
       "       'length_of_stay', 'crime_factor'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 141,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compas_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f-7G84vi3Z0N"
   },
   "source": [
    "# Binary Classification (1 output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "STiHStTHRjPu"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ga1tFxp2ZWQY",
    "outputId": "037259e2-506c-45bd-e8fd-934c7c13237b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 143,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train.dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "k37npOgNZBqC"
   },
   "outputs": [],
   "source": [
    "model = DNN_model(input_size=len(dataset_train.dataset[0][0]), hidden_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "muiQcJ3xPtSs"
   },
   "outputs": [],
   "source": [
    "#model = DNN(in_features=len(dataset_train.dataset[0][0]), hidden=[128,128]).to(device)\n",
    "NUM_EPOCHS = 50\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "criterion=nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVmzpYH5BKme",
    "outputId": "b334cedd-bd64-4093-e225-16236f24339a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN_model(\n",
       "  (fc1): Linear(in_features=6, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "oZQuIIsSQ4wZ"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    start_time = time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        optimizer.zero_grad()   # .backward() accumulates gradients\n",
    "        data = data.to(device)\n",
    "        target = target.to(device) # all data & model on same device\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 30000 == 0:\n",
    "          print(\"Runnning loss \", batch_idx, \" : \", running_loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time()\n",
    "    \n",
    "    running_loss /= len(train_loader)\n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
    "    return running_loss\n",
    "\n",
    "def test_model(model, test_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):   \n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            outputs = model(data)\n",
    "            target = target.flatten()\n",
    "            outputs = outputs.flatten()\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            pred = outputs.cpu().detach()\n",
    "            pred[pred >= 0.5] = 1\n",
    "            pred[pred < 0.5] = 0\n",
    "            correct_predictions += (pred == target).sum().item()\n",
    "            total_predictions += target.size(0)\n",
    "\n",
    "\n",
    "        running_loss /= len(test_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Testing Loss: ', running_loss)\n",
    "        print('Testing Accuracy: ', acc, '%')\n",
    "        return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sIPAlbwfRcyW",
    "outputId": "6ae56388-72f5-40b5-f9b8-5bc2b64408de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no: 0\n",
      "Runnning loss  0  :  0.5306012034416199\n",
      "Training Loss:  0.5127270340919494 Time:  0.04605579376220703 s\n",
      "Testing Loss:  0.537648253970676\n",
      "Testing Accuracy:  71.40287769784173 %\n",
      "====================\n",
      "epoch no: 1\n",
      "Runnning loss  0  :  0.5115630626678467\n",
      "Training Loss:  0.5090706646442413 Time:  0.13956975936889648 s\n",
      "Testing Loss:  0.5355472730265723\n",
      "Testing Accuracy:  72.48201438848922 %\n",
      "====================\n",
      "epoch no: 2\n",
      "Runnning loss  0  :  0.5195796489715576\n",
      "Training Loss:  0.5092786014080047 Time:  0.0521693229675293 s\n",
      "Testing Loss:  0.5339545177088844\n",
      "Testing Accuracy:  72.3021582733813 %\n",
      "====================\n",
      "epoch no: 3\n",
      "Runnning loss  0  :  0.5147454738616943\n",
      "Training Loss:  0.5074390172958374 Time:  0.05163288116455078 s\n",
      "Testing Loss:  0.5378566715452406\n",
      "Testing Accuracy:  71.58273381294964 %\n",
      "====================\n",
      "epoch no: 4\n",
      "Runnning loss  0  :  0.5145432949066162\n",
      "Training Loss:  0.5048848032951355 Time:  0.055703163146972656 s\n",
      "Testing Loss:  0.5368463297684988\n",
      "Testing Accuracy:  71.0431654676259 %\n",
      "====================\n",
      "epoch no: 5\n",
      "Runnning loss  0  :  0.5095219612121582\n",
      "Training Loss:  0.5051406025886536 Time:  0.05597329139709473 s\n",
      "Testing Loss:  0.5320697840717103\n",
      "Testing Accuracy:  72.66187050359713 %\n",
      "====================\n",
      "epoch no: 6\n",
      "Runnning loss  0  :  0.5084169507026672\n",
      "Training Loss:  0.5038587987422943 Time:  0.04709005355834961 s\n",
      "Testing Loss:  0.5304024848673079\n",
      "Testing Accuracy:  72.12230215827337 %\n",
      "====================\n",
      "epoch no: 7\n",
      "Runnning loss  0  :  0.5077165961265564\n",
      "Training Loss:  0.5022732377052307 Time:  0.049617767333984375 s\n",
      "Testing Loss:  0.5470197548468908\n",
      "Testing Accuracy:  72.48201438848922 %\n",
      "====================\n",
      "epoch no: 8\n",
      "Runnning loss  0  :  0.49498558044433594\n",
      "Training Loss:  0.5022583961486816 Time:  0.05847477912902832 s\n",
      "Testing Loss:  0.5300559136602614\n",
      "Testing Accuracy:  71.94244604316546 %\n",
      "====================\n",
      "epoch no: 9\n",
      "Runnning loss  0  :  0.503597617149353\n",
      "Training Loss:  0.4993995726108551 Time:  0.04624462127685547 s\n",
      "Testing Loss:  0.5344903634654151\n",
      "Testing Accuracy:  72.84172661870504 %\n",
      "====================\n",
      "epoch no: 10\n",
      "Runnning loss  0  :  0.4995805323123932\n",
      "Training Loss:  0.502448844909668 Time:  0.05484294891357422 s\n",
      "Testing Loss:  0.5353677421808243\n",
      "Testing Accuracy:  71.40287769784173 %\n",
      "====================\n",
      "epoch no: 11\n",
      "Runnning loss  0  :  0.4997938275337219\n",
      "Training Loss:  0.4973947048187256 Time:  0.04799818992614746 s\n",
      "Testing Loss:  0.5365099294318093\n",
      "Testing Accuracy:  71.76258992805755 %\n",
      "====================\n",
      "epoch no: 12\n",
      "Runnning loss  0  :  0.4854194223880768\n",
      "Training Loss:  0.5021842122077942 Time:  0.048922061920166016 s\n",
      "Testing Loss:  0.5270125584469901\n",
      "Testing Accuracy:  73.38129496402878 %\n",
      "====================\n",
      "epoch no: 13\n",
      "Runnning loss  0  :  0.49906080961227417\n",
      "Training Loss:  0.49573280215263366 Time:  0.13599371910095215 s\n",
      "Testing Loss:  0.5258641044298807\n",
      "Testing Accuracy:  73.20143884892086 %\n",
      "====================\n",
      "epoch no: 14\n",
      "Runnning loss  0  :  0.4889533817768097\n",
      "Training Loss:  0.49323907494544983 Time:  0.044328927993774414 s\n",
      "Testing Loss:  0.5348310718933741\n",
      "Testing Accuracy:  71.94244604316546 %\n",
      "====================\n",
      "epoch no: 15\n",
      "Runnning loss  0  :  0.5213642716407776\n",
      "Training Loss:  0.49404115080833433 Time:  0.04518723487854004 s\n",
      "Testing Loss:  0.5281536512904697\n",
      "Testing Accuracy:  72.84172661870504 %\n",
      "====================\n",
      "epoch no: 16\n",
      "Runnning loss  0  :  0.5115648508071899\n",
      "Training Loss:  0.4907860100269318 Time:  0.04693961143493652 s\n",
      "Testing Loss:  0.5272878342204623\n",
      "Testing Accuracy:  71.94244604316546 %\n",
      "====================\n",
      "epoch no: 17\n",
      "Runnning loss  0  :  0.475164532661438\n",
      "Training Loss:  0.4915654718875885 Time:  0.05127120018005371 s\n",
      "Testing Loss:  0.5248183243804507\n",
      "Testing Accuracy:  71.94244604316546 %\n",
      "====================\n",
      "epoch no: 18\n",
      "Runnning loss  0  :  0.4720548391342163\n",
      "Training Loss:  0.49265060424804685 Time:  0.04586458206176758 s\n",
      "Testing Loss:  0.5380395650863647\n",
      "Testing Accuracy:  72.48201438848922 %\n",
      "====================\n",
      "epoch no: 19\n",
      "Runnning loss  0  :  0.5168809294700623\n",
      "Training Loss:  0.49555320739746095 Time:  0.04804348945617676 s\n",
      "Testing Loss:  0.5548940880431069\n",
      "Testing Accuracy:  72.84172661870504 %\n",
      "====================\n",
      "epoch no: 20\n",
      "Runnning loss  0  :  0.5208706855773926\n",
      "Training Loss:  0.4954250633716583 Time:  0.04538464546203613 s\n",
      "Testing Loss:  0.5236336787541708\n",
      "Testing Accuracy:  72.66187050359713 %\n",
      "====================\n",
      "epoch no: 21\n",
      "Runnning loss  0  :  0.49469566345214844\n",
      "Training Loss:  0.4946229577064514 Time:  0.046245574951171875 s\n",
      "Testing Loss:  0.5283711734745238\n",
      "Testing Accuracy:  74.10071942446042 %\n",
      "====================\n",
      "epoch no: 22\n",
      "Runnning loss  0  :  0.48566198348999023\n",
      "Training Loss:  0.49110119342803954 Time:  0.04924321174621582 s\n",
      "Testing Loss:  0.5261812971697913\n",
      "Testing Accuracy:  72.84172661870504 %\n",
      "====================\n",
      "epoch no: 23\n",
      "Runnning loss  0  :  0.4685555696487427\n",
      "Training Loss:  0.4939147651195526 Time:  0.048865318298339844 s\n",
      "Testing Loss:  0.5327700475851694\n",
      "Testing Accuracy:  72.3021582733813 %\n",
      "====================\n",
      "epoch no: 24\n",
      "Runnning loss  0  :  0.4744667708873749\n",
      "Training Loss:  0.4888187825679779 Time:  0.15809082984924316 s\n",
      "Testing Loss:  0.5443347477250629\n",
      "Testing Accuracy:  73.7410071942446 %\n",
      "====================\n",
      "epoch no: 25\n",
      "Runnning loss  0  :  0.5074895620346069\n",
      "Training Loss:  0.4886826634407043 Time:  0.057444095611572266 s\n",
      "Testing Loss:  0.5252452625168694\n",
      "Testing Accuracy:  72.12230215827337 %\n",
      "====================\n",
      "epoch no: 26\n",
      "Runnning loss  0  :  0.4628603756427765\n",
      "Training Loss:  0.4864941954612732 Time:  0.056970834732055664 s\n",
      "Testing Loss:  0.5278958595461316\n",
      "Testing Accuracy:  72.84172661870504 %\n",
      "====================\n",
      "epoch no: 27\n",
      "Runnning loss  0  :  0.49951842427253723\n",
      "Training Loss:  0.4861553370952606 Time:  0.053215980529785156 s\n",
      "Testing Loss:  0.5348320106665293\n",
      "Testing Accuracy:  72.84172661870504 %\n",
      "====================\n",
      "epoch no: 28\n",
      "Runnning loss  0  :  0.5009884238243103\n",
      "Training Loss:  0.4870867133140564 Time:  0.046509504318237305 s\n",
      "Testing Loss:  0.5243114133675894\n",
      "Testing Accuracy:  73.02158273381295 %\n",
      "====================\n",
      "epoch no: 29\n",
      "Runnning loss  0  :  0.4901954233646393\n",
      "Training Loss:  0.4843154430389404 Time:  0.04941534996032715 s\n",
      "Testing Loss:  0.5242428332567215\n",
      "Testing Accuracy:  72.12230215827337 %\n",
      "====================\n",
      "epoch no: 30\n",
      "Runnning loss  0  :  0.4903510510921478\n",
      "Training Loss:  0.4870088458061218 Time:  0.04507946968078613 s\n",
      "Testing Loss:  0.5229161861870024\n",
      "Testing Accuracy:  72.12230215827337 %\n",
      "====================\n",
      "epoch no: 31\n",
      "Runnning loss  0  :  0.48186516761779785\n",
      "Training Loss:  0.48398866057395934 Time:  0.04581856727600098 s\n",
      "Testing Loss:  0.5270537535349528\n",
      "Testing Accuracy:  72.48201438848922 %\n",
      "====================\n",
      "epoch no: 32\n",
      "Runnning loss  0  :  0.4819526672363281\n",
      "Training Loss:  0.48380260467529296 Time:  0.04330730438232422 s\n",
      "Testing Loss:  0.5243270728323195\n",
      "Testing Accuracy:  72.66187050359713 %\n",
      "====================\n",
      "epoch no: 33\n",
      "Runnning loss  0  :  0.489553838968277\n",
      "Training Loss:  0.4866828262805939 Time:  0.04717373847961426 s\n",
      "Testing Loss:  0.5193407055404451\n",
      "Testing Accuracy:  73.38129496402878 %\n",
      "====================\n",
      "epoch no: 34\n",
      "Runnning loss  0  :  0.4868099093437195\n",
      "Training Loss:  0.4885033667087555 Time:  0.04423356056213379 s\n",
      "Testing Loss:  0.5202535208728578\n",
      "Testing Accuracy:  73.20143884892086 %\n",
      "====================\n",
      "epoch no: 35\n",
      "Runnning loss  0  :  0.4639655351638794\n",
      "Training Loss:  0.48730125427246096 Time:  0.04370522499084473 s\n",
      "Testing Loss:  0.5228692756758796\n",
      "Testing Accuracy:  72.3021582733813 %\n",
      "====================\n",
      "epoch no: 36\n",
      "Runnning loss  0  :  0.48591944575309753\n",
      "Training Loss:  0.48448890447616577 Time:  0.14639019966125488 s\n",
      "Testing Loss:  0.5236977554029889\n",
      "Testing Accuracy:  72.48201438848922 %\n",
      "====================\n",
      "epoch no: 37\n",
      "Runnning loss  0  :  0.4939628839492798\n",
      "Training Loss:  0.4817908227443695 Time:  0.05047488212585449 s\n",
      "Testing Loss:  0.522380005982187\n",
      "Testing Accuracy:  74.46043165467626 %\n",
      "====================\n",
      "epoch no: 38\n",
      "Runnning loss  0  :  0.49102360010147095\n",
      "Training Loss:  0.48165047764778135 Time:  0.05276179313659668 s\n",
      "Testing Loss:  0.5254161920812395\n",
      "Testing Accuracy:  72.48201438848922 %\n",
      "====================\n",
      "epoch no: 39\n",
      "Runnning loss  0  :  0.4884374737739563\n",
      "Training Loss:  0.48259761929512024 Time:  0.05263543128967285 s\n",
      "Testing Loss:  0.5223603728744719\n",
      "Testing Accuracy:  73.38129496402878 %\n",
      "====================\n",
      "epoch no: 40\n",
      "Runnning loss  0  :  0.4806026816368103\n",
      "Training Loss:  0.4827300369739532 Time:  0.05522322654724121 s\n",
      "Testing Loss:  0.5202701919608645\n",
      "Testing Accuracy:  73.56115107913669 %\n",
      "====================\n",
      "epoch no: 41\n",
      "Runnning loss  0  :  0.47808346152305603\n",
      "Training Loss:  0.47993154525756837 Time:  0.05490899085998535 s\n",
      "Testing Loss:  0.5197419060601128\n",
      "Testing Accuracy:  73.20143884892086 %\n",
      "====================\n",
      "epoch no: 42\n",
      "Runnning loss  0  :  0.4769534766674042\n",
      "Training Loss:  0.48135467767715456 Time:  0.046544790267944336 s\n",
      "Testing Loss:  0.5197859323687024\n",
      "Testing Accuracy:  74.10071942446042 %\n",
      "====================\n",
      "epoch no: 43\n",
      "Runnning loss  0  :  0.460689514875412\n",
      "Training Loss:  0.48201284408569334 Time:  0.04846525192260742 s\n",
      "Testing Loss:  0.5209325287077162\n",
      "Testing Accuracy:  73.92086330935251 %\n",
      "====================\n",
      "epoch no: 44\n",
      "Runnning loss  0  :  0.4764232933521271\n",
      "Training Loss:  0.48462793231010437 Time:  0.04099464416503906 s\n",
      "Testing Loss:  0.5229874319500394\n",
      "Testing Accuracy:  74.10071942446042 %\n",
      "====================\n",
      "epoch no: 45\n",
      "Runnning loss  0  :  0.4636945128440857\n",
      "Training Loss:  0.4839873433113098 Time:  0.04852151870727539 s\n",
      "Testing Loss:  0.5187627358569039\n",
      "Testing Accuracy:  74.46043165467626 %\n",
      "====================\n",
      "epoch no: 46\n",
      "Runnning loss  0  :  0.47882264852523804\n",
      "Training Loss:  0.482613205909729 Time:  0.05938434600830078 s\n",
      "Testing Loss:  0.5232279168234931\n",
      "Testing Accuracy:  73.02158273381295 %\n",
      "====================\n",
      "epoch no: 47\n",
      "Runnning loss  0  :  0.4897898733615875\n",
      "Training Loss:  0.48345971703529356 Time:  0.04495429992675781 s\n",
      "Testing Loss:  0.5279999905162387\n",
      "Testing Accuracy:  73.92086330935251 %\n",
      "====================\n",
      "epoch no: 48\n",
      "Runnning loss  0  :  0.5088217258453369\n",
      "Training Loss:  0.48208550810813905 Time:  0.1547384262084961 s\n",
      "Testing Loss:  0.522789306110806\n",
      "Testing Accuracy:  73.92086330935251 %\n",
      "====================\n",
      "epoch no: 49\n",
      "Runnning loss  0  :  0.4727739989757538\n",
      "Training Loss:  0.4787606358528137 Time:  0.05992698669433594 s\n",
      "Testing Loss:  0.5197010553545423\n",
      "Testing Accuracy:  74.64028776978418 %\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "Train_loss = []\n",
    "Test_loss = []\n",
    "Test_acc = []\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    print(\"epoch no:\", i)\n",
    "    train_loss = train_epoch(model, dataset_train, criterion, optimizer)\n",
    "    test_loss, test_acc = test_model(model, dataset_val, criterion)\n",
    "    scheduler.step(test_loss)\n",
    "    Train_loss.append(train_loss)\n",
    "    Test_loss.append(test_loss)\n",
    "    Test_acc.append(test_acc)\n",
    "    print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgeIQpBWVvxP",
    "outputId": "961a0a86-c9e2-4385-e187-c382621f0f12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC of model : 0.7604640192126254\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "Y_pred, Y = [], []\n",
    "for features, targets in dataset_test:\n",
    "  y_pred = model(features).cpu().detach()\n",
    "  y_pred[y_pred >= 0.5] = 1\n",
    "  y_pred[y_pred < 0.5] = 0\n",
    "  Y_pred.append(y_pred)\n",
    "  Y.append(targets)\n",
    "Y_pred = torch.cat(Y_pred)\n",
    "Y = torch.cat(Y)\n",
    "print(\"AUC of model :\", roc_auc_score(np.array(Y),np.array(Y_pred)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DNN_COMPAS.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
